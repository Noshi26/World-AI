{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol3yFtDYtxVC"
   },
   "source": [
    "### Automated News Data Pipeline: Targeted Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31173,
     "status": "ok",
     "timestamp": 1751033565059,
     "user": {
      "displayName": "Nowshin Mosharaf",
      "userId": "00328869860970096266"
     },
     "user_tz": -360
    },
    "id": "eovNwTEG4x_t",
    "outputId": "83629e3b-6586-418d-e279-17157c8a255b"
   },
   "outputs": [],
   "source": [
    "# Block 1: WorldWatch AI - Focused Categorical Pipeline (V15.0 - Corrected)\n",
    "# Purpose: This script uses targeted keyword searches to collect news ONLY for\n",
    "# specific categories (War, Peace, Economic, Trade, Tech). Irrelevant news is never fetched.\n",
    "# It runs for a set duration and then stops automatically.\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "\n",
    "# <<< MODIFICATION: Replaced broad endpoints with specific, targeted keyword queries.\n",
    "# This is the core of the correction. We now ONLY ask the API for relevant news.\n",
    "CATEGORIZED_QUERIES = {\n",
    "    \"War\": \"war OR conflict OR invasion OR military action OR airstrike OR geopolitics\",\n",
    "    \"Peace\": \"peace talks OR treaty OR diplomacy OR ceasefire OR negotiations\",\n",
    "    \"Economic\": \"economy OR inflation OR recession OR interest rates OR gdp OR market crash\",\n",
    "    \"Trade\": \"trade deal OR tariff OR supply chain OR exports OR imports\",\n",
    "    \"Tech\": \"AI OR artificial intelligence OR semiconductor OR cybersecurity OR tech giant\"\n",
    "}\n",
    "\n",
    "MY_API_KEY = \"1ddd3a86143c475cb371f487a3617cbc\" # Your NewsAPI key\n",
    "DATASET_FILE = 'News_2025.csv'\n",
    "UPDATE_INTERVAL_SECONDS = 10\n",
    "RUN_DURATION_SECONDS = 30\n",
    "\n",
    "# --- 3. HELPER FUNCTION ---\n",
    "def fetch_live_news_from_url(api_key, url):\n",
    "    \"\"\"Fetches and minimally processes news from a single NewsAPI URL.\"\"\"\n",
    "    headers = {'X-Api-Key': api_key}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            articles = response.json().get('articles', [])\n",
    "            if not articles: return pd.DataFrame()\n",
    "            df = pd.DataFrame(articles)\n",
    "            df.rename(columns={'url': 'link', 'urlToImage': 'image'}, inplace=True)\n",
    "            df['source'] = df['source'].apply(lambda s: s.get('name') if isinstance(s, dict) else s)\n",
    "            # We now also want 'description' for context, and the new 'category' column will be added later.\n",
    "            required_cols = ['title', 'source', 'publishedAt', 'description', 'link', 'image']\n",
    "\n",
    "            # Ensure all columns exist to prevent errors, even if empty.\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = ''\n",
    "\n",
    "            df = df.dropna(subset=['title', 'publishedAt'])\n",
    "            return df[required_cols]\n",
    "        else:\n",
    "            print(f\"   âš ï¸ API Error (Status: {response.status_code})\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   âŒ Network Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 4. THE PIPELINE FUNCTION ---\n",
    "def run_data_pipeline():\n",
    "    \"\"\"\n",
    "    This function runs for a set duration, fetching news ONLY for the predefined\n",
    "    categories and saving them to the master CSV file.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    cycle_count = 0\n",
    "\n",
    "    try:\n",
    "        while time.time() - start_time < RUN_DURATION_SECONDS:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            cycle_count += 1\n",
    "            clear_output(wait=True)\n",
    "            print(f\"--- [ WorldWatch AI Focused Pipeline | V15.0 ] ---\")\n",
    "            print(f\"Running for {RUN_DURATION_SECONDS}s. Time elapsed: {int(elapsed_time)}s\")\n",
    "            print(f\"Starting Cycle {cycle_count} at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # === Step 1: Load Existing Dataset ===\n",
    "            try:\n",
    "                if os.path.exists(DATASET_FILE):\n",
    "                    local_df = pd.read_csv(DATASET_FILE)\n",
    "                    existing_titles = set(local_df['title']) if 'title' in local_df.columns else set()\n",
    "                    print(f\"âœ… Loaded local dataset '{DATASET_FILE}' with {len(local_df)} articles.\")\n",
    "                else:\n",
    "                    local_df = pd.DataFrame()\n",
    "                    existing_titles = set()\n",
    "                    print(f\"â“˜ Local dataset not found. A new '{DATASET_FILE}' will be created.\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ CRITICAL ERROR: Could not read local dataset. Error: {e}\")\n",
    "                time.sleep(UPDATE_INTERVAL_SECONDS)\n",
    "                continue\n",
    "\n",
    "            # === Step 2: Fetch Live News by PREDEFINED CATEGORY ===\n",
    "            # <<< MODIFICATION: The fetching logic is completely new and targeted.\n",
    "            print(\"\\nðŸ”Ž Fetching targeted news from API...\")\n",
    "            live_articles_dfs = []\n",
    "            for category, query in CATEGORIZED_QUERIES.items():\n",
    "                print(f\"   Querying for: '{category}'\")\n",
    "                # Build the specific URL for the query using the powerful /everything endpoint\n",
    "                url = f\"https://newsapi.org/v2/everything?q=({query})&language=en&sortBy=publishedAt\"\n",
    "\n",
    "                new_df = fetch_live_news_from_url(MY_API_KEY, url)\n",
    "\n",
    "                if new_df is not None and not new_df.empty:\n",
    "                    # IMPORTANT: We add the 'category' tag to the data we just fetched.\n",
    "                    new_df['category'] = category\n",
    "                    live_articles_dfs.append(new_df)\n",
    "                time.sleep(1) # Be polite to the API server\n",
    "\n",
    "            # === Step 3: Identify NEW Articles ===\n",
    "            newly_added_articles = []\n",
    "            if live_articles_dfs:\n",
    "                live_df_combined = pd.concat(live_articles_dfs, ignore_index=True)\n",
    "                live_df_combined.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "\n",
    "                for _, article in live_df_combined.iterrows():\n",
    "                    if article['title'] not in existing_titles:\n",
    "                        newly_added_articles.append(article)\n",
    "\n",
    "            # === Step 4: Report Findings and Save ===\n",
    "            if newly_added_articles:\n",
    "                new_articles_df = pd.DataFrame(newly_added_articles)\n",
    "                print(f\"\\nâœ… SUCCESS: Found {len(new_articles_df)} new unique, categorized articles!\")\n",
    "\n",
    "                updated_df = pd.concat([local_df, new_articles_df], ignore_index=True)\n",
    "                updated_df.sort_values(by='publishedAt', ascending=False, inplace=True)\n",
    "\n",
    "                # <<< MODIFICATION: Ensure a clean, ordered final CSV with the new category column\n",
    "                final_cols = ['category', 'title', 'source', 'publishedAt', 'description', 'link', 'image']\n",
    "                # Make sure all columns exist, even if the old file didn't have them\n",
    "                for col in final_cols:\n",
    "                    if col not in updated_df.columns:\n",
    "                        updated_df[col] = ''\n",
    "                updated_df = updated_df[final_cols]\n",
    "\n",
    "                try:\n",
    "                    updated_df.to_csv(DATASET_FILE, index=False)\n",
    "                    print(f\"ðŸ’¾ Dataset '{DATASET_FILE}' successfully updated. Total articles: {len(updated_df)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ CRITICAL ERROR: Could not save updated dataset! Error: {e}\")\n",
    "            else:\n",
    "                print(\"\\nâœ… API fetch cycle complete. No new unique articles were found for the specified categories.\")\n",
    "\n",
    "            # === Step 5: Wait for the next cycle (if time permits) ===\n",
    "            print(\"-\" * 50)\n",
    "            if time.time() - start_time < RUN_DURATION_SECONDS:\n",
    "                 print(f\"Cycle {cycle_count} complete. Waiting for {UPDATE_INTERVAL_SECONDS} seconds...\")\n",
    "                 time.sleep(UPDATE_INTERVAL_SECONDS)\n",
    "\n",
    "        print(\"\\n\\n--- [ Pipeline Complete ] ---\")\n",
    "        print(f\"Process finished automatically after the configured runtime.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n--- [ Pipeline Shutdown ] ---\")\n",
    "        print(\"Data collection process stopped by user.\")\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw2-N6LDuQh-"
   },
   "source": [
    "### WorldWatch AI: AI Engine Training for News Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21310,
     "status": "ok",
     "timestamp": 1751033028524,
     "user": {
      "displayName": "Nowshin Mosharaf",
      "userId": "00328869860970096266"
     },
     "user_tz": -360
    },
    "id": "Wg4c1Xi3zhbB",
    "outputId": "696f9b71-4a5a-4ce7-b00c-7af1c09b67ca"
   },
   "outputs": [],
   "source": [
    "# Block 2: The AI Search & Optimization Engine (Complete Edition)\n",
    "# Purpose: To demonstrate AI search algorithms and use a Genetic Algorithm\n",
    "# to train a complete, 10-category model for use in the live dashboard.\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "!pip install -q pygad\n",
    "import pandas as pd\n",
    "import pygad\n",
    "import numpy as np\n",
    "import re\n",
    "import heapq\n",
    "import pprint\n",
    "\n",
    "# --- 2. LOAD & PREPARE TRAINING DATA ---\n",
    "try:\n",
    "    df_static = pd.read_csv('News_2025.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'News_2025.csv' not found. Please upload it to your Colab session.\")\n",
    "    raise\n",
    "\n",
    "ground_truth = {0: 'Tech', 1: 'War', 2: 'Crime/Justice', 3: 'Science', 4: 'Environment', 5: 'Economic', 6: 'War', 7: 'Peace', 8: 'Trade', 9: 'Economic', 10: 'War', 11: 'War', 12: 'Tech', 13: 'Economic', 14: 'Peace'}\n",
    "sample_df = df_static.iloc[0:15].copy()\n",
    "sample_df['true_category'] = sample_df.index.map(ground_truth)\n",
    "print(f\"Loaded and prepared training sample of {len(sample_df)} articles.\")\n",
    "\n",
    "# --- 3. UNIVERSAL CLASSIFICATION FUNCTION ---\n",
    "def classify_headline(headline, keyword_db):\n",
    "    headline_lower = headline.lower()\n",
    "    scores = {category: 0 for category in keyword_db}\n",
    "    for category, keywords in keyword_db.items():\n",
    "        for keyword, score in keywords.items():\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', headline_lower):\n",
    "                scores[category] += score\n",
    "    if all(s == 0 for s in scores.values()): return 'Neutral'\n",
    "    # Important: Return the category with the highest ABSOLUTE score\n",
    "    return max(scores, key=lambda cat: abs(scores.get(cat, 0)))\n",
    "\n",
    "def calculate_accuracy(keyword_db):\n",
    "    predictions = sample_df['title'].apply(lambda h: classify_headline(h, keyword_db))\n",
    "    correct = (predictions == sample_df['true_category']).sum()\n",
    "    return correct / len(sample_df)\n",
    "\n",
    "# =====================================================================================\n",
    "# SECTION A: PATHFINDING ALGORITHMS (DEMONSTRATION)\n",
    "# Note: This section uses a simplified graph to demonstrate the concept of pathfinding.\n",
    "# =====================================================================================\n",
    "print(\"\\n--- [Section A: Pathfinding Algorithms Demo] ---\")\n",
    "CANDIDATE_MODELS = {'Initial':{'war':10,'peace':-5,'economy':3}, 'High-Risk':{'war':15,'peace':-2,'economy':5}, 'Balanced':{'war':8,'peace':-8,'economy':4}, 'Peace-Focused':{'war':5,'peace':-15,'economy':1}, 'Best (Goal)':{'war':9,'peace':-7,'economy':6}}\n",
    "for name, model_data in CANDIDATE_MODELS.items(): CANDIDATE_MODELS[name] = {'War':{'war': model_data['war']}, 'Peace':{'peace': model_data['peace']}, 'Economic':{'economy': model_data['economy']}}\n",
    "def get_neighbors(model_name):\n",
    "    connections = {'Initial': ['High-Risk','Balanced'],'High-Risk':['Initial','Best (Goal)'],'Balanced':['Initial','Peace-Focused'],'Peace-Focused':['Balanced','Best (Goal)'],'Best (Goal)':['High-Risk','Peace-Focused']}\n",
    "    return connections.get(model_name, [])\n",
    "def heuristic(model_name): return 1.0 - calculate_accuracy(CANDIDATE_MODELS[model_name])\n",
    "def search_algorithm(start, goal, use_heuristic=False, use_path_cost=False):\n",
    "    frontier = [(0, 0, [start])]; explored = set()\n",
    "    while frontier:\n",
    "        priority, cost, path = heapq.heappop(frontier)\n",
    "        current = path[-1]\n",
    "        if current in explored: continue\n",
    "        explored.add(current)\n",
    "        if current == goal: return path\n",
    "        for neighbor in get_neighbors(current):\n",
    "            if neighbor not in explored:\n",
    "                new_cost = cost + 1; h = heuristic(neighbor) if use_heuristic else 0; c = new_cost if use_path_cost else 0\n",
    "                heapq.heappush(frontier, (c + h, new_cost, path + [neighbor]))\n",
    "    return None\n",
    "\n",
    "print(\"1. Running UCS -> Path Found:\", ' -> '.join(search_algorithm('Initial', 'Best (Goal)', use_path_cost=True)))\n",
    "print(\"2. Running Greedy -> Path Found:\", ' -> '.join(search_algorithm('Initial', 'Best (Goal)', use_heuristic=True)))\n",
    "print(\"3. Running A* -> Path Found:\", ' -> '.join(search_algorithm('Initial', 'Best (Goal)', use_path_cost=True, use_heuristic=True)))\n",
    "\n",
    "# =====================================================================================\n",
    "# SECTION B: LOCAL SEARCH (HILL CLIMBING - DEMONSTRATION)\n",
    "# =====================================================================================\n",
    "print(\"\\n--- [Section B: Hill Climbing Demo] ---\")\n",
    "def hill_climbing(max_iterations=500):\n",
    "    current_solution = {'War':{'war':10},'Peace':{'peace':-5},'Economic':{'economy':3}}\n",
    "    current_accuracy = calculate_accuracy(current_solution)\n",
    "    for _ in range(max_iterations):\n",
    "        neighbor = {c:{k:v for k,v in kws.items()} for c,kws in current_solution.items()}\n",
    "        cat, kw = 'War','war'\n",
    "        neighbor[cat][kw] += np.random.normal(0, 1.5)\n",
    "        if calculate_accuracy(neighbor) > current_accuracy:\n",
    "            current_solution, current_accuracy = neighbor, calculate_accuracy(neighbor)\n",
    "    return current_accuracy\n",
    "print(f\"1. Hill Climbing finished. Best accuracy found: {hill_climbing():.2%}\")\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# SECTION C: GLOBAL OPTIMIZATION (GENETIC ALGORITHM - FULL TRAINING)\n",
    "# This is the primary training engine. It uses the full, 10-category knowledge base.\n",
    "# =====================================================================================\n",
    "print(\"\\n--- [Section C: Genetic Algorithm - Full Model Training] ---\")\n",
    "\n",
    "# --- THIS IS THE UPDATED, FULL KNOWLEDGE BASE ---\n",
    "FULL_INITIAL_KEYWORDS = {\n",
    "    'War': {'war': 10, 'attack': 9, 'conflict': 8, 'strike': 9, 'military': 7, 'invasion': 10, 'casualties': 8, 'airstrike': 9, 'troops': 6, 'battle': 8, 'weapon': 7, 'missile': 9},\n",
    "    'Peace': {'peace': -5, 'treaty': -7, 'agreement': -6, 'ceasefire': -8, 'truce': -8, 'negotiation': -4, 'diplomacy': -5, 'reconciliation': -6, 'summit': -4},\n",
    "    'Economic': {'economy': 3, 'inflation': 6, 'recession': 7, 'market': 4, 'growth': -2, 'gdp': 3, 'stock': 4, 'unemployment': 7, 'jobs': -3, 'fed': 5},\n",
    "    'Trade': {'trade': 2, 'tariff': 5, 'sanction': 6, 'export': 1, 'import': 1, 'supply chain': 4, 'deal': -3, 'commerce': 2, 'embargo': 7},\n",
    "    'Tech': {'technology': 1, 'ai': 2, 'cybersecurity': 6, 'innovation': -1, 'space': 1, 'semiconductor': 4, 'data': 3, 'software': 1, 'apple': 2},\n",
    "    'Politics': {'election': 4, 'vote': 3, 'government': 3, 'congress': 4, 'president': 5, 'protest': 6, 'policy': 3, 'scandal': 6, 'impeachment': 8},\n",
    "    'Health': {'health': 2, 'virus': 7, 'pandemic': 8, 'vaccine': -2, 'fda': 4, 'who': 3, 'outbreak': 7, 'disease': 6, 'hospital': 4, 'cdc': 4},\n",
    "    'Environment': {'climate': 6.0, 'global warming': 7, 'emissions': 5, 'renewable': -3, 'solar': -2, 'pollution': 6, 'wildfire': 8, 'disaster': 8},\n",
    "    'Crime/Justice': {'crime': 6, 'police': 4, 'arrest': 5, 'court': 4, 'trial': 5, 'lawsuit': 4, 'justice': 2, 'fbi': 5, 'prison': 5},\n",
    "    'Science': {'science': 1, 'discovery': -1, 'research': 1, 'study': 1, 'nasa': 1, 'breakthrough': -2, 'experiment': 2}\n",
    "}\n",
    "\n",
    "def fitness_func_full(ga_instance, solution, solution_idx):\n",
    "    temp_keywords = {}\n",
    "    i = 0\n",
    "    for category, keywords in FULL_INITIAL_KEYWORDS.items():\n",
    "        temp_keywords[category] = {}\n",
    "        for keyword in keywords:\n",
    "            temp_keywords[category][keyword] = solution[i]\n",
    "            i += 1\n",
    "    return calculate_accuracy(temp_keywords)\n",
    "\n",
    "initial_weights = [score for cat_kw in FULL_INITIAL_KEYWORDS.values() for score in cat_kw.values()]\n",
    "ga_instance = pygad.GA(num_generations=100, num_parents_mating=7, fitness_func=fitness_func_full,\n",
    "                       sol_per_pop=20, num_genes=len(initial_weights), init_range_low=-15.0,\n",
    "                       init_range_high=15.0, parent_selection_type=\"sss\", keep_parents=2,\n",
    "                       crossover_type=\"single_point\", mutation_type=\"random\", mutation_percent_genes=8,\n",
    "                       suppress_warnings=True)\n",
    "\n",
    "print(\"1. Running Genetic Algorithm to find optimal weights for the FULL 10-category model...\")\n",
    "ga_instance.run()\n",
    "best_solution, best_solution_fitness, _ = ga_instance.best_solution()\n",
    "print(f\"   - Optimization complete. Best accuracy on sample data: {best_solution_fitness:.2%}\")\n",
    "\n",
    "# --- 7. FINAL OUTPUT ---\n",
    "OPTIMIZED_KEYWORDS = {}\n",
    "i = 0\n",
    "for category, keywords in FULL_INITIAL_KEYWORDS.items():\n",
    "    OPTIMIZED_KEYWORDS[category] = {}\n",
    "    for keyword in keywords:\n",
    "        OPTIMIZED_KEYWORDS[category][keyword] = best_solution[i]\n",
    "        i += 1\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"--- ENGINE COMPLETE: OPTIMIZED AI MODEL GENERATED ---\")\n",
    "print(\"--- Copy the dictionary below and paste it into Block 2 ---\")\n",
    "print(\"=\"*60)\n",
    "pprint.pprint(OPTIMIZED_KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWpliOawud6v"
   },
   "source": [
    "### WorldWatch AI Dashboard: Live Categorized News View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "92efa2f0a3d54c6bafd6a69b64261c1c",
      "5c479ad76dd4416e963d29688192144f",
      "ebb5350587e444c1bf04807daad8e550",
      "fb2aeb534d9c420991959ec05762b6db",
      "4f9b67e6ea634249b15c277e307277d7",
      "5f5b59bba9ac40f59be06aae8a7c420a",
      "266f00078ea8431f99fff2911a4425fd",
      "2e6fc232e628425f847d5c20a2397700",
      "98811d805f6a47bda9ed5af6c87304e2",
      "329f40bdbd744a56a5f0e0e0f8347bfb",
      "a5b2410dbe894ff4806f8f1b1f1da9f3",
      "456c3826598f4db0bc0e93b795de5ff5",
      "51c4c467660d4efb97ee25ec53a28363",
      "bbf1d58e778b449998927b9f682226ac",
      "2e85033196e74351a17610b4bccde2b1",
      "13528d8c88c748a79c29d16fc5db99a3",
      "852d5efe5ffd47c08f862abbfb5d26e4",
      "f1a52e11ba1a4bf3a3b73af1b0bdf7c2",
      "9d3c381efbcd40c58c3f1b47e4b58d8a",
      "d61f9cca7d8b46379b1bde6294551e7d",
      "4b4ff7d03f3545f3a503b36c6920e439",
      "c7da271d6ec9422db471b30ce38f09d8",
      "0c3290f0e5ed47668b042fb4638480bf",
      "2d7cb93266d3422fb7d4e169a1a2afeb",
      "e9106b9aab794e1d8c88093a4f8f1b8e",
      "ad9f169015184a4fa908bf2b7b7939b9",
      "5fd31f340ddf48459abaefbac759d618",
      "37b6651f135c4cd8aa151e0707fa05af",
      "c6119249e23442d68cac4545875ee893",
      "3957614f213f48e4ae2d6a5e3d215f48",
      "a2c2734d97184aa7aa6b0297a4ed91ea",
      "c2b2f5a8bb0940a7b2e2c21ad1cd1753",
      "ef18968446af47c8aaf2703289c81ec8",
      "873dc0ebb2604dcfb25749425bdf4c23",
      "96578c14de2843d7ae419bfeb2b82549",
      "4f31cd319d284d19967acacdb6345e0a",
      "d7bf0ae52037402b8dc553abf724044b",
      "0d7d63eb9fe84bc0b5be7fd59f9d9f81",
      "be72aae25fee43468c5075e4371922fc",
      "a7d9d63796174e3487b11fa1ff21cd51",
      "5f9ad37e189e438a8f2194bdd5c70c60",
      "7ee865c635764151b43ae0ab6d4a93fc",
      "3b9ede3eb1e74934a0e93c5983022b39",
      "5a448fc9dfe44927aa9ea601301ad952",
      "9cbb4a58d7f3449cabb92f70c1ff2719"
     ]
    },
    "executionInfo": {
     "elapsed": 4800,
     "status": "ok",
     "timestamp": 1751033490776,
     "user": {
      "displayName": "Nowshin Mosharaf",
      "userId": "00328869860970096266"
     },
     "user_tz": -360
    },
    "id": "2hz_dyNw9lrE",
    "outputId": "988eec81-05d8-4c02-d52c-7d1aa01e9b67"
   },
   "outputs": [],
   "source": [
    "# Block 3: WorldWatch AI Dashboard (V21.0 - High-Precision Final Edition)\n",
    "# Purpose: The definitive dashboard with highly-targeted, contextual queries to\n",
    "# ensure the \"War\" and \"Economic\" feeds are clean, relevant, and professional.\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Layout\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# --- 2. CONFIGURATION & AI MODELS ---\n",
    "\n",
    "# --- AI Model for Classification ---\n",
    "OPTIMIZED_KEYWORDS = {\n",
    "    'Economic': {'bonds': -4.56, 'budget': 7.63, 'downturn': 14.28, 'economy': 11.23, 'fed': 13.78, 'gdp': 9.22, 'growth': 8.52, 'inflation': 14.88, 'jobs': 12.01, 'market': 12.07, 'rates': 14.93, 'recession': 14.89, 'stimulus': 11.19, 'stock': 12.98, 'unemployment': 14.53},\n",
    "    'Peace': {'accord': 8.84, 'agreement': 14.15, 'ceasefire': 14.93, 'de-escalation': 14.39, 'diplomacy': 14.99, 'diplomatic': 14.97, 'mediation': 4.13, 'negotiation': 14.89, 'peace': 15.0, 'summit': 14.92, 'treaty': 14.97, 'truce': 14.88, 'un': -4.27},\n",
    "    'Tech': {'ai': 14.24, 'algorithm': 9.0, 'amazon': -0.11, 'apple': 14.82, 'cybersecurity': 14.97, 'data': 12.18, 'google': 14.96, 'innovation': 14.11, 'launch': 8.32, 'meta': 14.49, 'metaverse': 14.77, 'microsoft': 14.91, 'nvidia': 14.92, 'semiconductor': 14.95, 'software': 12.01, 'tech': 14.86},\n",
    "    'Trade': {'commerce': 14.98, 'deal': 14.95, 'embargo': 14.12, 'export': 14.13, 'goods': 14.22, 'import': 14.83, 'logistics': 12.0, 'ports': 10.39, 'sanction': 14.93, 'shipping': 11.33, 'supply chain': 14.99, 'tariff': 14.99, 'trade': 14.99, 'wto': 8.21},\n",
    "    'War': {'airstrike': 14.95, 'artillery': 14.89, 'attack': 14.95, 'battle': 14.91, 'casualties': 15.0, 'conflict': 14.99, 'defense': 14.92, 'drone': 14.54, 'escalation': 14.8, 'geopolitical': 14.42, 'invasion': 14.97, 'military': 14.98, 'missile': 14.99, 'offensive': 14.86, 'strike': 15.0, 'troops': 14.98, 'weapon': 14.85}\n",
    "}\n",
    "\n",
    "# --- Data Collection Pipeline Configuration ---\n",
    "# <<< MODIFICATION: This is the definitive correction for your entire project.\n",
    "# These queries are now highly specific and contextual.\n",
    "CATEGORIZED_QUERIES = {\n",
    "    # War: Must mention a specific actor AND a specific action.\n",
    "    \"War\": '(\"Israel\" OR \"Iran\" OR \"USA\" OR \"Gaza\" OR \"Hezbollah\" OR \"Palestine\") AND (attack OR war OR strike OR missile OR conflict OR airstrike OR invasion OR offensive)',\n",
    "\n",
    "    # Economic: Must mention an economic term AND be in the context of the conflict.\n",
    "    \"Economic\": '(\"economy\" OR \"inflation\" OR \"oil prices\" OR \"supply chain\" OR \"stock market\" OR \"recession\" OR \"sanctions\") AND (\"Israel\" OR \"Iran\" OR \"Gaza\" OR \"Middle East\")',\n",
    "\n",
    "    # Peace: Remains broad but focused on diplomatic terms.\n",
    "    \"Peace\": \"peace talks OR treaty OR diplomacy OR ceasefire OR negotiations OR UN resolution\",\n",
    "\n",
    "    # Trade & Tech: Remain broad as they are less prone to irrelevant noise.\n",
    "    \"Trade\": \"trade deal OR tariff OR supply chain OR exports OR imports OR wto\",\n",
    "    \"Tech\": \"AI OR artificial intelligence OR semiconductor OR cybersecurity OR tech giant\"\n",
    "}\n",
    "MY_API_KEY = \"1ddd3a86143c475cb371f487a3617cbc\"\n",
    "\n",
    "# --- UI Configuration ---\n",
    "COLORS = {\n",
    "    'War': '#ff6b6b', 'Peace': '#63e6be', 'Economic': '#fcc419', 'Trade': '#ff922b', 'Tech': '#4dabf7', 'Neutral': '#adb5bd',\n",
    "    'background': '#f8f9fa', 'card_bg': '#ffffff', 'text_primary': '#212529', 'text_secondary': '#868e96',\n",
    "    'shadow': 'rgba(0, 0, 0, 0.05)', 'border': '#dee2e6',\n",
    "    'button_primary': '#A7D9E0', 'button_primary_hover': '#97C8CF',\n",
    "    'header_bg_start': '#CDECF2', 'header_bg_end': '#A7D9E0',\n",
    "    'header_text_main': '#014a60', 'header_text_sub': '#027899',\n",
    "    'status_info_bg': '#e7f5ff', 'status_info_border': '#a5d8ff', 'status_info_text': '#1c7ed6',\n",
    "    'status_success_bg': '#e6fcf5', 'status_success_border': '#b2f2bb', 'status_success_text': '#2f9e44',\n",
    "    'status_error_bg': '#fff5f5', 'status_error_border': '#ffc9c9', 'status_error_text': '#e03131'\n",
    "}\n",
    "TAB_CATEGORIES = {'All Headlines': 'globe','War': 'fighter-jet','Peace': 'handshake-o','Economic': 'line-chart','Trade': 'exchange','Tech': 'cogs'}\n",
    "FINAL_DF_SCHEMA = ['title', 'source', 'publishedAt', 'link', 'image', 'category', 'score']\n",
    "DATASET_FILE = 'News_2025.csv'\n",
    "\n",
    "\n",
    "# --- 3. DASHBOARD & PIPELINE FUNCTIONS ---\n",
    "# (The Python logic functions from here are unchanged as they adapt to the new data)\n",
    "status_area = widgets.Output()\n",
    "\n",
    "def show_status_message(message, style_class, icon):\n",
    "    status_area.clear_output(wait=True)\n",
    "    with status_area:\n",
    "        display(HTML(f\"\"\"<div class=\"status-banner {style_class}\"><i class=\"fa {icon} status-icon\"></i><span class=\"status-message\">{message}</span></div>\"\"\"))\n",
    "\n",
    "def fetch_live_news_from_url(api_key, url):\n",
    "    headers = {'X-Api-Key': api_key}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            articles = response.json().get('articles', [])\n",
    "            if not articles: return pd.DataFrame()\n",
    "            df = pd.DataFrame(articles)\n",
    "            df.rename(columns={'url': 'link', 'urlToImage': 'image'}, inplace=True)\n",
    "            df['source'] = df['source'].apply(lambda s: s.get('name') if isinstance(s, dict) else s)\n",
    "            required_cols = ['title', 'source', 'publishedAt', 'description', 'link', 'image']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns: df[col] = ''\n",
    "            return df.dropna(subset=['title', 'publishedAt'])[required_cols]\n",
    "        return None\n",
    "    except requests.exceptions.RequestException: return None\n",
    "\n",
    "def run_live_data_collection():\n",
    "    show_status_message(\"Synchronizing with high-precision news feeds...\", \"status-info\", \"fa-hourglass-half\")\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        local_df = pd.read_csv(DATASET_FILE) if os.path.exists(DATASET_FILE) else pd.DataFrame()\n",
    "        existing_titles = set(local_df['title']) if 'title' in local_df.columns else set()\n",
    "        live_articles_dfs = []\n",
    "        for category, query in CATEGORIZED_QUERIES.items():\n",
    "            encoded_query = quote(query)\n",
    "            url = f\"https://newsapi.org/v2/everything?q={encoded_query}&language=en&sortBy=publishedAt\"\n",
    "            new_df = fetch_live_news_from_url(MY_API_KEY, url)\n",
    "            if new_df is not None and not new_df.empty:\n",
    "                new_df['category'] = category\n",
    "                live_articles_dfs.append(new_df)\n",
    "            time.sleep(0.5)\n",
    "        if not live_articles_dfs:\n",
    "            show_status_message(\"No new articles were found matching the specific criteria. Feeds are monitored.\", \"status-info\", \"fa-info-circle\")\n",
    "            return\n",
    "        live_df_combined = pd.concat(live_articles_dfs, ignore_index=True).drop_duplicates(subset=['title'], keep='first')\n",
    "        newly_added_articles = [article for _, article in live_df_combined.iterrows() if article['title'] not in existing_titles]\n",
    "        if newly_added_articles:\n",
    "            show_status_message(f\"Found and added {len(newly_added_articles)} new, highly relevant articles.\", \"status-success\", \"fa-check-circle\")\n",
    "            updated_df = pd.concat([local_df, pd.DataFrame(newly_added_articles)], ignore_index=True)\n",
    "            updated_df.sort_values(by='publishedAt', ascending=False, inplace=True).to_csv(DATASET_FILE, index=False)\n",
    "        else:\n",
    "            show_status_message(\"All feeds checked. Your dataset is already up-to-date with the latest intelligence.\", \"status-success\", \"fa-check-circle\")\n",
    "        time.sleep(1.5)\n",
    "    except Exception as e:\n",
    "        show_status_message(f\"An error occurred during data collection: {e}\", \"status-error\", \"fa-exclamation-triangle\")\n",
    "\n",
    "def classify_and_score(headline, keyword_db):\n",
    "    headline_lower, scores = str(headline).lower(), {category: 0 for category in keyword_db}\n",
    "    for category, keywords in keyword_db.items():\n",
    "        for keyword, score in keywords.items():\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', headline_lower): scores[category] += score\n",
    "    if all(s == 0 for s in scores.values()): return 'Neutral', 0\n",
    "    top_category = max(scores, key=scores.get)\n",
    "    return top_category, round(scores[top_category], 1)\n",
    "\n",
    "def render_article_card(article):\n",
    "    category, color = article.get('category', 'Neutral'), COLORS.get(article.get('category', 'Neutral'), COLORS['Neutral'])\n",
    "    image_url = article.get('image', '')\n",
    "    img_html = f'<div class=\"card-image-container\"><img class=\"card-image\" src=\"{image_url}\" alt=\"Image not available\" onerror=\"this.parentElement.classList.add(\\'no-image\\');\"></div>' if image_url and isinstance(image_url, str) else '<div class=\"card-image-container no-image\"></div>'\n",
    "    return f\"\"\"<a href=\"{article['link']}\" target=\"_blank\" class=\"article-card-link\"><div class=\"article-card\" style=\"border-left-color: {color};\">{img_html}<div class=\"card-content\"><div class=\"card-category-tag\" style=\"background-color: {color};\">{category.upper()}</div><h3 class=\"card-title\">{article['title']}</h3><div class=\"card-footer\"><span class=\"card-source\">{article.get('source', 'Unknown Source')}</span><span class=\"card-published\">{article['publishedAt'].strftime('%b %d, %Y')}</span></div></div></div></a>\"\"\"\n",
    "\n",
    "def create_content_view(df, status_message=\"\"):\n",
    "    if df.empty: return f\"\"\"<div class=\"empty-state\"><i class=\"fa fa-search\" style=\"font-size: 4em; color: #ced4da;\"></i><h2>{status_message}</h2><p>No articles match the current filter. Try clicking Refresh.</p></div>\"\"\"\n",
    "    cards_html = \"\".join([render_article_card(row) for _, row in df.iterrows()])\n",
    "    return f\"<div class='articles-grid'>{cards_html}</div>\"\n",
    "\n",
    "master_df, active_tab_name, output_area = pd.DataFrame(columns=FINAL_DF_SCHEMA), 'All Headlines', widgets.Output()\n",
    "def update_display_area(clear_status=False):\n",
    "    output_area.clear_output(wait=True)\n",
    "    if clear_status: status_area.clear_output()\n",
    "    query = search_input.value.strip().lower()\n",
    "    df_to_process = master_df\n",
    "    if query: df_to_process = master_df[master_df['title'].str.lower().str.contains(query, na=False)]\n",
    "    if active_tab_name == 'All Headlines':\n",
    "        priority_categories = ['War', 'Economic', 'Tech']\n",
    "        df_for_view = df_to_process[df_to_process['category'].isin(priority_categories)]\n",
    "        status_msg = f\"Showing {len(df_for_view)} priority articles\"\n",
    "        empty_msg = \"No articles found for War, Economic, or Tech.\"\n",
    "    else:\n",
    "        df_for_view = df_to_process[df_to_process['category'] == active_tab_name]\n",
    "        status_msg = f\"Showing {len(df_for_view)} articles in '{active_tab_name}'\"\n",
    "        empty_msg = f\"No articles found for '{active_tab_name}'\"\n",
    "    content_html = create_content_view(df_for_view, empty_msg)\n",
    "    footer = f\"\"\"<div class=\"dashboard-footer\"><p>WorldWatch AI Live Edition</p><p class=\"footer-separator\">|</p><p>{status_msg}</p><p class=\"footer-separator\">|</p><p>Last update: {datetime.now().strftime('%H:%M:%S')}</p></div>\"\"\"\n",
    "    with output_area: display(HTML(f\"<div class='fade-in'>{content_html}{footer}</div>\"))\n",
    "\n",
    "def handle_refresh_click(b=None):\n",
    "    global master_df, active_tab_name\n",
    "    output_area.clear_output(wait=True)\n",
    "    with output_area: display(HTML('<div class=\"loader-container\"><div class=\"loader\"></div><h3>Preparing Dashboard...</h3></div>'))\n",
    "    run_live_data_collection()\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        master_df = pd.DataFrame(columns=FINAL_DF_SCHEMA)\n",
    "        update_display_area(clear_status=True)\n",
    "        return\n",
    "    try:\n",
    "        temp_df = pd.read_csv(DATASET_FILE)\n",
    "        temp_df['link'] = temp_df['link'].fillna('')\n",
    "        temp_df['publishedAt'] = pd.to_datetime(temp_df['publishedAt'], errors='coerce', utc=True).dt.tz_localize(None)\n",
    "        temp_df.dropna(subset=['title', 'publishedAt'], inplace=True)\n",
    "        results = temp_df['title'].apply(lambda h: classify_and_score(h, OPTIMIZED_KEYWORDS)).tolist()\n",
    "        temp_df[['category', 'score']] = pd.DataFrame(results, index=temp_df.index)\n",
    "        master_df = temp_df[FINAL_DF_SCHEMA].copy().sort_values(by='publishedAt', ascending=False)\n",
    "        search_input.value, active_tab_name = '', 'All Headlines'\n",
    "        for btn in tab_buttons: btn.button_style = 'info' if btn.description == active_tab_name else ''\n",
    "        update_display_area()\n",
    "    except Exception as e:\n",
    "        show_status_message(f\"A critical error occurred while processing the dataset: {e}\", \"status-error\", \"fa-exclamation-triangle\")\n",
    "\n",
    "def on_filter_change(b):\n",
    "    global active_tab_name\n",
    "    if isinstance(b, widgets.Button) and b.description in TAB_CATEGORIES:\n",
    "        active_tab_name = b.description\n",
    "        for btn in tab_buttons: btn.button_style = 'info' if btn.description == active_tab_name else ''\n",
    "    update_display_area(clear_status=True)\n",
    "\n",
    "# --- 5. UI LAYOUT ---\n",
    "global_css = widgets.HTML(f\"\"\"<style>@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n",
    ":root {{ --main-font: 'Inter', sans-serif; --bg: {COLORS['background']}; --card-bg: {COLORS['card_bg']}; --text1: {COLORS['text_primary']}; --text2: {COLORS['text_secondary']}; --border: {COLORS['border']}; --shadow: {COLORS['shadow']}; }}\n",
    ".dashboard-header {{ background: linear-gradient(135deg, {COLORS['header_bg_start']} 0%, {COLORS['header_bg_end']} 100%); border-bottom: 1px solid {COLORS['button_primary_hover']}; padding: 30px 20px; text-align: center; border-radius: 12px; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);}}\n",
    ".dashboard-header h1 {{ margin: 0; font-size: 2.5em; font-weight: 700; color: {COLORS['header_text_main']}; }}\n",
    ".dashboard-header p {{ font-size: 1.1em; margin-top: 5px; color: {COLORS['header_text_sub']}; font-weight: 500; }}\n",
    ".jupyter-button.mod-primary {{ background-color: {COLORS['button_primary']} !important; border: 1px solid {COLORS['button_primary_hover']} !important; color: #212529 !important; }}\n",
    ".jupyter-button.mod-primary:hover {{ background-color: {COLORS['button_primary_hover']} !important; }}\n",
    ".jupyter-widgets.widget-text input[type=\"text\"]:focus {{ border-color: {COLORS['Tech']}; box-shadow: 0 0 0 3px {COLORS['Tech']}33; }}\n",
    ".articles-grid {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(320px, 1fr)); gap: 25px; }}\n",
    ".article-card-link {{ text-decoration: none; display: flex; }}\n",
    ".article-card {{ background: var(--card-bg); border-radius: 12px; box-shadow: 0 4px 20px var(--shadow); border-left: 5px solid; transition: all 0.25s ease; display: flex; flex-direction: column; width: 100%; }}\n",
    ".article-card:hover {{ transform: translateY(-5px); box-shadow: 0 8px 25px rgba(0,0,0,0.08); }}\n",
    ".card-image-container {{ width: 100%; aspect-ratio: 16 / 9; background-color: #f1f3f5; border-radius: 10px 10px 0 0; overflow: hidden; }}\n",
    ".card-image-container.no-image {{ background-image: url('https://www.gstatic.com/images/icons/material/system/2x/photo_camera_grey600_48dp.png'); background-size: 48px; background-position: center; background-repeat: no-repeat; }}\n",
    ".card-image {{ width: 100%; height: 100%; object-fit: cover; }}\n",
    ".card-content {{ padding: 20px; display: flex; flex-direction: column; flex-grow: 1; }}\n",
    ".card-category-tag {{ padding: 4px 10px; border-radius: 6px; font-weight: 600; color: white; font-size: 0.75em; display: inline-block; margin-bottom: 12px; }}\n",
    ".card-title {{ font-size: 1.1em; font-weight: 600; color: var(--text1); margin: 0 0 15px 0; line-height: 1.45; flex-grow: 1; }}\n",
    ".card-footer {{ display: flex; justify-content: space-between; align-items: center; font-size: 0.85em; color: var(--text2); border-top: 1px solid var(--border); padding-top: 15px; margin-top: auto; }}\n",
    ".loader {{ height: 50px; width: 50px; margin: 0 auto 20px auto; border: 5px solid #e9ecef; border-top-color: {COLORS['Tech']}; border-radius: 50%; animation: spin 1s linear infinite; }} @keyframes spin {{ to {{ transform: rotate(360deg); }} }}\n",
    ".status-banner {{ padding: 15px 20px; margin: 0 0 25px 0; border: 1px solid; border-radius: 8px; display: flex; align-items: center; animation: fadeInDown 0.5s ease-out; }}\n",
    ".status-icon {{ font-size: 1.4em; margin-right: 15px; }}\n",
    ".status-message {{ font-size: 1em; font-weight: 500; }}\n",
    ".status-info {{ background-color: {COLORS['status_info_bg']}; border-color: {COLORS['status_info_border']}; color: {COLORS['status_info_text']}; }}\n",
    ".status-success {{ background-color: {COLORS['status_success_bg']}; border-color: {COLORS['status_success_border']}; color: {COLORS['status_success_text']}; }}\n",
    ".status-error {{ background-color: {COLORS['status_error_bg']}; border-color: {COLORS['status_error_border']}; color: {COLORS['status_error_text']}; }}\n",
    "@keyframes fadeInDown {{ from {{ opacity: 0; transform: translateY(-10px); }} to {{ opacity: 1; transform: translateY(0); }} }}\n",
    ".dashboard-footer {{ display: flex; justify-content: center; align-items: center; gap: 15px; padding: 20px; color: var(--text2); font-size: 0.9em; border-top: 1px solid var(--border); margin-top: 30px; }}\n",
    ".footer-separator {{ color: var(--border); }}\n",
    "</style>\"\"\")\n",
    "header = widgets.HTML(f\"\"\"<div class=\"dashboard-header\"><h1>WorldWatch AI</h1><p>Your Global Intelligence Command Center</p></div>\"\"\")\n",
    "refresh_button = widgets.Button(description=\"Fetch & Refresh Data\", icon='refresh', button_style='primary', layout=Layout(width='auto', margin='0 15px 0 0'))\n",
    "search_input = widgets.Text(placeholder='Search articles...', layout=Layout(width='auto', flex='1 1 0'))\n",
    "search_button = widgets.Button(icon='search', layout=Layout(width='auto'))\n",
    "tab_buttons = [widgets.Button(description=t, icon=i, layout=Layout(min_width='90px', width='auto', margin='2px')) for t,i in TAB_CATEGORIES.items()]\n",
    "if tab_buttons: tab_buttons[0].button_style = 'info'\n",
    "refresh_button.on_click(handle_refresh_click)\n",
    "search_input.on_submit(on_filter_change)\n",
    "search_button.on_click(on_filter_change)\n",
    "for btn in tab_buttons: btn.on_click(on_filter_change)\n",
    "top_controls = HBox([refresh_button, search_input, search_button], layout=Layout(width='100%', justify_content='center', align_items='center'))\n",
    "control_panel = VBox([top_controls, HBox(tab_buttons, layout=Layout(justify_content='center', margin='15px 0 0 0', flex_wrap='wrap'))], layout=Layout(width='100%', margin='0 0 20px 0'))\n",
    "app_layout = VBox([global_css, header, control_panel, status_area, output_area], layout=Layout(width='98%', max_width='1400px', margin='20px auto'))\n",
    "\n",
    "# --- 6. LAUNCH DASHBOARD ---\n",
    "display(app_layout)\n",
    "handle_refresh_click()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/JSurzq076hu00ywMhxfH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
